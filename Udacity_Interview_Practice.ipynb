{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "outputExpanded": false
   },
   "source": [
    "# Udacity CAREER DEVELOPMENT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "outputExpanded": false
   },
   "source": [
    "## &#x1F3E2; &nbsp; Interview Practice (Data Analyst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "outputExpanded": false
   },
   "source": [
    "###  &#x1F4D1; &nbsp; 1. \n",
    "#### Describe a data project you worked on recently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "outputExpanded": false
   },
   "source": [
    "The last project was connected to the Data Wrangling Udacity course. I chose the map sector of the dynamically developing area in the UAE, downloaded a XML OSM dataset, audited and cleaned the dataset, converting it from XML to JSON format, imported the .json file into a MongoDB database, and explored it by running queries.\n",
    "\n",
    "Problems and errors in the dataset:\n",
    "- One of the main problems of public maps - no duplication of all place names in other languages. If it were possible to automate the translation process by increasing a common database of map names in many languages, it would save users from many difficulties and mistakes.\n",
    "- The next problem - the presence of a large number of databases (including mapping) on the same map objects. Some integrating procedures of already available data would relieve a lot of people from unnecessary work, save time and effort.\n",
    "- Obviously, the information about the number of buildings and their purpose is incomplete. Completeness of public maps can be increased by bringing in the process of mapping new users. For this goal enter the information should be as simple as possible: for example, a choice of the available options with automatic filling many fields for linked options (for example, linking the name of the street and the administrative area in which it is located).\n",
    "- There are a number of mistakes and typos as in every public data. For correction, well-known methods can be proposed: automatic comparison with existing data and verification for new data by other users.\n",
    "- During working on the project, I spent a lot of time on the conversion of one type of data file to another. Each format has its own advantages and disadvantages. Probably, it is possible to design a universal file type that allows us to store data of any kind, combining the advantages of all existing types and applicable in the most of existing programming languages.\n",
    "- Correction of errors made in the data seems to me appropriate to carry out after uploading files to the database. Sometimes a record that is a mistake in terms of filling a particular type of data just contains additional information about geo-objects.\n",
    "\n",
    "Difficulties in the project for the processing of data in the language Python and MongoDB data: loading data into MongoDB database occurred only partly due to the upgrade of the software environment MacBook Sierra. To prepare for the implementation of the project in all circumstances I have mastered all possible ways: the processing of data in the borders of SQL database in the language Python, as well as processing of the data (SQL and MongoDB) in the language R. It gave me 4 ways for the project in general.\n",
    "\n",
    "Of course, I could just change the PC, but it was the learning project exactly for improvement our skills. So I preferred this decision. Eventually, I have a good level of practice skills in software packages to support data science applications and I can hope to find a job as a data analyst and a Python or R programmer at the same time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "outputExpanded": false
   },
   "source": [
    "###  &#x1F4D1; &nbsp; 2.\n",
    "#### You are given a ten piece box of chocolate truffles. You know based on the label that six of the pieces have an orange cream filling and four of the pieces have a coconut filling. If you were to eat four pieces in a row, what is the probability that the first two pieces you eat have an orange cream filling and the last two have a coconut filling?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "outputExpanded": false
   },
   "source": [
    "Let us denote O - a piece of candy with orange cream and C - with coconut filling. \n",
    "\n",
    "The probability that the first is a sweet with orange filling is 6/10 (for the condition of the problem in a box of sweets 6 of 10 have this stuffing). The probability that the next will be a sweet with orange cream is 5/9 (in a box of sweets now we have 9 in total, 5 of them - with orange filling). Arguing similarly, we obtain the probability for the third and the fourth candy to have a coconut filling.\n",
    "\n",
    "At concurrence of all four provisions of sweets we'll get a favorable event, so we multiply the probabilities of all values obtained.\n",
    "\n",
    "The favorable outcome and its probability will be as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "outputExpanded": false
   },
   "source": [
    "### OOCC (6/10)x(5/9)x(4/8)x(3/7) = 1/14 ≈ 0.0714"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "outputExpanded": false
   },
   "source": [
    "#### Follow-up question: If you were given an identical box of chocolates and again eat four pieces in a row, what is the probability that exactly two contain coconut filling?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "outputExpanded": false
   },
   "source": [
    "In this case, all of these are favorable events."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "outputExpanded": false
   },
   "source": [
    "### CCOO (4/10)x(3/9)x(6/8)x(5/7) = 1/14\n",
    "### COCO (4/10)x(6/9)x(3/8)x(5/7) = 1/14\n",
    "### COOC (4/10)x(6/9)x(5/8)x(3/7) = 1/14\n",
    "### OCCO (6/10)x(4/9)x(3/8)x(5/7) = 1/14\n",
    "### OCOC (6/10)x(4/9)x(5/8)x(3/7) = 1/14\n",
    "### OOCC (6/10)x(5/9)x(4/8)x(3/7) = 1/14"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "outputExpanded": false
   },
   "source": [
    "To obtain the required probability we have to add up all the fractions.\n",
    "### 6/14 = 3/7 ≈ 0.4286"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "outputExpanded": false
   },
   "source": [
    "###  &#x1F4D1; &nbsp; 3. \n",
    "#### Given the table users:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "outputExpanded": false
   },
   "outputs": [],
   "source": [
    "     Table \"users\"\n",
    "+-------------+-----------+\n",
    "| Column      | Type      |\n",
    "+-------------+-----------+\n",
    "| id          | integer   |\n",
    "| username    | character |\n",
    "| email       | character |\n",
    "| city        | character |\n",
    "| state       | character |\n",
    "| zip         | integer   |\n",
    "| active      | boolean   |\n",
    "+-------------+-----------+"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "outputExpanded": false
   },
   "source": [
    "#### construct a query to find the top 5 states with the highest number of active users. Include the number for each state in the query result. Example result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "outputExpanded": false
   },
   "outputs": [],
   "source": [
    "+------------+------------------+\n",
    "| state      | num_active_users |\n",
    "+------------+------------------+\n",
    "| New Mexico | 502              |\n",
    "| Alabama    | 495              |\n",
    "| California | 300              |\n",
    "| Maine      | 201              |\n",
    "| Texas      | 189              |\n",
    "+------------+------------------+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "outputExpanded": false
   },
   "outputs": [],
   "source": [
    "# SQL (sqlite3)\n",
    "# code row #1 Select from the column \"state\" and count the users\n",
    "# code row #2 Use the table \"users\"\n",
    "# code row #3 Chose the data point only if the value in the column \"active\" = true (1 for SQL)\n",
    "# code row #4 Group by values in the column \"state\"\n",
    "# code row #5 List in order from the biggest to the smallest\n",
    "# code row #6 Show on 5 fist notes\n",
    "c.execute(\"SELECT state, COUNT(*) as num_active_users \\ \n",
    "           FROM users \\\n",
    "           WHERE active = 1 \\\n",
    "           GROUP BY state \\\n",
    "           ORDER BY num_active_users DESC \\\n",
    "           LIMIT 5;\")\n",
    "print c.fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "outputExpanded": false
   },
   "outputs": [],
   "source": [
    "# MongoDB (pymongo)\n",
    "# The same procedure for other type of the database\n",
    "users.aggregate( [ { \"$match\" : { \"active\" : True } }, \n",
    "                   { \"$group\" : { \"_id\" : \"$state\", \"num_active_users\" : { \"$sum\" : 1} } },  \n",
    "                   { \"$sort\" : { \"num_active_users\" : -1}}, {\"$limit\": 5}] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "outputExpanded": false
   },
   "source": [
    "###  &#x1F4D1; &nbsp; 4. \n",
    "#### Define a function first_unique that takes a string as input and returns the first non-repeated (unique) character in the input string. If there are no unique characters return None. Note: Your code should be in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "outputExpanded": false
   },
   "outputs": [],
   "source": [
    "def first_unique(string):\n",
    "# Your code here\n",
    " return unique_char\n",
    "\n",
    "> first_unique('aabbcdd123')\n",
    "> c\n",
    "\n",
    "> first_unique('a')\n",
    "> a\n",
    "\n",
    "> first_unique('112233')\n",
    "> None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "outputExpanded": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result",
     "text": []
    }
   ],
   "source": [
    "'aabbcdd123'.count('d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false,
    "outputExpanded": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "c\n",
      "a\n"
     ]
    }
   ],
   "source": [
    "def first_unique(string):\n",
    "    for letter in string:    # Loop for every character in the string\n",
    "        if string.count(letter) == 1:    # Check if the amount of this character in the string is equal to 1 or not\n",
    "            return letter    # Show the character if the condition has the value «true»\n",
    "        else:\n",
    "            continue    # Continue to check the next symbol if the condition has the value «false»\n",
    "    return None    # Return «None» if the condition has the value «false» for every character\n",
    "\n",
    "print first_unique('112233')\n",
    "print first_unique('aabbcdd123')\n",
    "print first_unique('a')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "outputExpanded": false
   },
   "source": [
    "###  &#x1F4D1; &nbsp; 5.\n",
    "#### What are underfitting and overfitting in the context of Machine Learning? How might you balance them?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "outputExpanded": false
   },
   "source": [
    "Underfitting and overfitting are two main reasons for non-effective work of machine learning algorithms. It leads to poor predictions for new data sets.\n",
    "\n",
    "Underfitting means constructing a model which does not work with the training data or any new data. In this case, the machine learning algorithm cannot capture the data trend. It happens because of simplifying the model.\n",
    "\n",
    "Overfitting means constructing a model which fits the training data too well and does not work with any new data. Here the machine learning algorithm can capture the trend of data noise (or the non-exist trend). It happens because of complicating the model.\n",
    "\n",
    "Underfitting is much easier to detect with a good performance metric. Overfitting can be overcome by using techniques to estimate model accuracy (for example, k-fold cross validation).\n",
    "\n",
    "An ideal fit in machine learning is to catch a moment between these two critical points. The best way to find a good balance is to increase (for underfitting) or to decrease(for overfitting) the flexibility of the model by adding new features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "outputExpanded": false
   },
   "source": [
    "###  &#x1F4D1; &nbsp; 6.\n",
    "#### If you were to start your data analyst position today, what would be your goals a year from now?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "outputExpanded": false
   },
   "source": [
    "The first goal is to find a job in this sphere.\n",
    "\n",
    "The next one is to complete at least one of the Nanodegree programs else: Machine Learning Engineer, Predictive Analytics for Business, Artificial Intelligence Engineer or VR Developer. Ideally, a good specialist in the modern data processing should complete all 5 Nanodegree courses. \n",
    "\n",
    "There are several important reasons for that.\n",
    "- This is one of the fastest growing areas in the labor market. Without constant work on the skills the candidate can not hope to get a good position.\n",
    "- A higher level of skills involves participation in the solution of the most interesting problems in the field of modern challenges (the creation of high technology for the processing of large data, the construction of virtual spaces for different purposes, the creation of a qualitatively new, preventive of the diseases, medicine, revolutionary changes in the natural sciences, the preservation of human values when creating different kind of artificial intelligence, etc.)\n",
    "- Excellent knowledge allows the candidate to show more creativity and flexibility in the process of working.\n",
    "\n",
    "As for me personally, I would like to qualitatively improve their skills in working with super large files, in the application of technologies JavaScript and HTML, in the construction of complex and interactive visualization, and the machine learning and then a year later  - in the building models of probability, natural language processing, computer vision."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "gist_id": "c302dd3875a0e05c4c4d39ce3796cb78",
  "hide_input": false,
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "latex_envs": {
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
