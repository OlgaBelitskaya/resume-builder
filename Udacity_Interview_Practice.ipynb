{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "outputExpanded": false
   },
   "source": [
    "# Udacity CAREER DEVELOPMENT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "outputExpanded": false
   },
   "source": [
    "## &#x1F3E2; &nbsp; Interview Practice (Data Analyst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "outputExpanded": false
   },
   "source": [
    "###  &#x1F4D1; &nbsp; 1. \n",
    "#### Describe a data project you worked on recently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "outputExpanded": false
   },
   "source": [
    "The last project was connected to the Data Wrangling Udacity course. I chose the map sector of the dynamically developing area in the UAE, downloaded a XML OSM dataset, audited and cleaned the dataset, converting it from XML to JSON format, imported the .json file into a MongoDB database, and explored it by running queries.\n",
    "\n",
    "Problems and errors in the dataset:\n",
    "- One of the main problems of public maps - no duplication of all place names in other languages. If it were possible to automate the translation process by increasing a common database of map names in many languages, it would save users from many difficulties and mistakes.\n",
    "- The next problem - the presence of a large number of databases (including mapping) on the same map objects. Some integrating procedures of already available data would relieve a lot of people from unnecessary work, save time and effort.\n",
    "- Obviously, the information about the number of buildings and their purpose is incomplete. Completeness of public maps can be increased by bringing in the process of mapping new users. For this goal enter the information should be as simple as possible: for example, a choice of the available options with automatic filling many fields for linked options (for example, linking the name of the street and the administrative area in which it is located).\n",
    "- There are a number of mistakes and typos as in every public data. For correction, well-known methods can be proposed: automatic comparison with existing data and verification for new data by other users.\n",
    "- During working on the project, I spent a lot of time on the conversion of one type of data file to another. Each format has its own advantages and disadvantages. Probably, it is possible to design a universal file type that allows us to store data of any kind, combining the advantages of all existing types and applicable in the most of existing programming languages.\n",
    "- Correction of errors made in the data seems to me appropriate to carry out after uploading files to the database. Sometimes a record that is a mistake in terms of filling a particular type of data just contains additional information about geo-objects.\n",
    "\n",
    "Difficulties in the project for the processing of data in the language Python and MongoDB data: loading data into MongoDB database occurred only partly due to the upgrade of the software environment MacBook Sierra. To prepare for the implementation of the project in all circumstances I have mastered several possible ways using various combinations of software: SQL - Python, MongoDB - Python, SQL - R, and MongoDB - R. It gave me confidence in the realization of the project in general.\n",
    "\n",
    "Of course, I could just change the PC, but it was the learning project exactly for improvement our skills. So I preferred this decision. Eventually, I have a good level of practice skills in software packages to support data science applications and I can hope to find a job as a data analyst and a Python or R programmer at the same time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "outputExpanded": false
   },
   "source": [
    "###  &#x1F4D1; &nbsp; 2.\n",
    "#### You are given a ten piece box of chocolate truffles. You know based on the label that six of the pieces have an orange cream filling and four of the pieces have a coconut filling. If you were to eat four pieces in a row, what is the probability that the first two pieces you eat have an orange cream filling and the last two have a coconut filling?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "outputExpanded": false
   },
   "source": [
    "Let us denote O - a piece of candy with orange cream and C - with coconut filling. \n",
    "\n",
    "The probability that the first is a sweet with orange filling is 6/10 (for the condition of the problem in a box of sweets 6 of 10 have this stuffing). The probability that the next will be a sweet with orange cream is 5/9 (in a box of sweets now we have 9 in total, 5 of them - with orange filling). Arguing similarly, we obtain the probability for the third and the fourth candy to have a coconut filling.\n",
    "\n",
    "At concurrence of all four provisions of sweets we'll get a favorable event, so we multiply the probabilities of all values obtained.\n",
    "\n",
    "The favorable outcome and its probability will be as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "outputExpanded": false
   },
   "source": [
    "### OOCC (6/10)x(5/9)x(4/8)x(3/7) = 1/14 ≈ 0.0714"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "outputExpanded": false
   },
   "source": [
    "#### Follow-up question: If you were given an identical box of chocolates and again eat four pieces in a row, what is the probability that exactly two contain coconut filling?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "outputExpanded": false
   },
   "source": [
    "In this case, all of these are favorable events."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "outputExpanded": false
   },
   "source": [
    "### CCOO (4/10)x(3/9)x(6/8)x(5/7) = 1/14\n",
    "### COCO (4/10)x(6/9)x(3/8)x(5/7) = 1/14\n",
    "### COOC (4/10)x(6/9)x(5/8)x(3/7) = 1/14\n",
    "### OCCO (6/10)x(4/9)x(3/8)x(5/7) = 1/14\n",
    "### OCOC (6/10)x(4/9)x(5/8)x(3/7) = 1/14\n",
    "### OOCC (6/10)x(5/9)x(4/8)x(3/7) = 1/14"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "outputExpanded": false
   },
   "source": [
    "To obtain the required probability we have to add up all the fractions.\n",
    "### 6/14 = 3/7 ≈ 0.4286"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "outputExpanded": false
   },
   "source": [
    "###  &#x1F4D1; &nbsp; 3. \n",
    "#### Given the table users:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "outputExpanded": false
   },
   "outputs": [],
   "source": [
    "     Table \"users\"\n",
    "+-------------+-----------+\n",
    "| Column      | Type      |\n",
    "+-------------+-----------+\n",
    "| id          | integer   |\n",
    "| username    | character |\n",
    "| email       | character |\n",
    "| city        | character |\n",
    "| state       | character |\n",
    "| zip         | integer   |\n",
    "| active      | boolean   |\n",
    "+-------------+-----------+"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "outputExpanded": false
   },
   "source": [
    "#### construct a query to find the top 5 states with the highest number of active users. Include the number for each state in the query result. Example result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "outputExpanded": false
   },
   "outputs": [],
   "source": [
    "+------------+------------------+\n",
    "| state      | num_active_users |\n",
    "+------------+------------------+\n",
    "| New Mexico | 502              |\n",
    "| Alabama    | 495              |\n",
    "| California | 300              |\n",
    "| Maine      | 201              |\n",
    "| Texas      | 189              |\n",
    "+------------+------------------+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "outputExpanded": false
   },
   "outputs": [],
   "source": [
    "# SQL (sqlite3)\n",
    "# code row #1 Select from the column \"state\" and count the users\n",
    "# code row #2 Use the table \"users\"\n",
    "# code row #3 Chose the data point only if the value in the column \"active\" = true (1 for SQL)\n",
    "# code row #4 Group by values in the column \"state\"\n",
    "# code row #5 List in order from the biggest to the smallest\n",
    "# code row #6 Show on 5 fist notes\n",
    "c.execute(\"SELECT state, COUNT(*) as num_active_users \\ \n",
    "           FROM users \\\n",
    "           WHERE active = 1 \\\n",
    "           GROUP BY state \\\n",
    "           ORDER BY num_active_users DESC \\\n",
    "           LIMIT 5;\")\n",
    "print c.fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "outputExpanded": false
   },
   "outputs": [],
   "source": [
    "# MongoDB (pymongo)\n",
    "# The same procedure for other type of the database\n",
    "users.aggregate( [ { \"$match\" : { \"active\" : True } }, \n",
    "                   { \"$group\" : { \"_id\" : \"$state\", \"num_active_users\" : { \"$sum\" : 1} } },  \n",
    "                   { \"$sort\" : { \"num_active_users\" : -1}}, {\"$limit\": 5}] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "outputExpanded": false
   },
   "source": [
    "###  &#x1F4D1; &nbsp; 4. \n",
    "#### Define a function first_unique that takes a string as input and returns the first non-repeated (unique) character in the input string. If there are no unique characters return None. Note: Your code should be in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "outputExpanded": false
   },
   "outputs": [],
   "source": [
    "def first_unique(string):\n",
    "# Your code here\n",
    " return unique_char\n",
    "\n",
    "> first_unique('aabbcdd123')\n",
    "> c\n",
    "\n",
    "> first_unique('a')\n",
    "> a\n",
    "\n",
    "> first_unique('112233')\n",
    "> None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "outputExpanded": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result",
     "text": []
    }
   ],
   "source": [
    "'aabbcdd123'.count('d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false,
    "outputExpanded": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "c\n",
      "a\n"
     ]
    }
   ],
   "source": [
    "def first_unique(string):\n",
    "    for letter in string:    # Loop for every character in the string\n",
    "        if string.count(letter) == 1:    # Check if the amount of this character in the string is equal to 1 or not\n",
    "            return letter    # Show the character if the condition has the value «true»\n",
    "        else:\n",
    "            continue    # Continue to check the next symbol if the condition has the value «false»\n",
    "    return None    # Return «None» if the condition has the value «false» for every character\n",
    "\n",
    "print first_unique('112233')\n",
    "print first_unique('aabbcdd123')\n",
    "print first_unique('a')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "outputExpanded": false
   },
   "source": [
    "###  &#x1F4D1; &nbsp; 5.\n",
    "#### What are underfitting and overfitting in the context of Machine Learning? How might you balance them?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "outputExpanded": false
   },
   "source": [
    "Underfitting and overfitting are two main reasons for non-effective work of machine learning algorithms. It leads to poor predictions for new data sets.\n",
    "\n",
    "Underfitting means constructing a model which does not work with the training data or any new data. In this case, the machine learning algorithm cannot capture the data trend. It happens because of simplifying the model.\n",
    "\n",
    "Overfitting means constructing a model which fits the training data too well and does not work with any new data. Here the machine learning algorithm can capture the trend of data noise (or the non-exist trend). It happens because of complicating the model.\n",
    "\n",
    "Underfitting is much easier to detect with a good performance metric. Overfitting can be overcome by using techniques to estimate model accuracy (for example, k-fold cross validation).\n",
    "\n",
    "An ideal fit in machine learning is to catch a moment between these two critical points. The best way to find a good balance is to increase (for underfitting) or to decrease(for overfitting) the flexibility of the model by adding new features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "outputExpanded": false
   },
   "source": [
    "###  &#x1F4D1; &nbsp; 6.\n",
    "#### If you were to start your data analyst position today, what would be your goals a year from now?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "outputExpanded": false
   },
   "source": [
    "The first goal is to find a job in this sphere. Of course, I use the package of helpful projects in the program \"Udacity Data Analyst Nanodegree\" in this process.\n",
    "\n",
    "To all career development projects (resume, cover letter, online profiles, interviews) were in the same vein, I used the description of one particular job for them \"Data Scientist and R Programmer\" https://www.r-users.com/jobs/data-scientist-and-r-programmer/. This vacancy at the moment is no longer relevant, but it is not as important as this interview is just for training. \n",
    "\n",
    "In the description of the position, responsibilities are clearly defined (prepare analysis-ready data, produce dynamic and static data visualizations, develop innovative formats for delivering scientific analysis results to clients, etc.). Admission requirements in matters of skills, education, and experience are listed. Prospects for the company are indicated. All these points gave me a very clear idea of the work ahead.\n",
    "\n",
    "In my career, I often had to drastically change the sphere of solving problems in building objects and their projects (for example, compliance issues of state standards, outdoor advertising signs, traffic management on the certain territory, the questions of registration of real estate, and so on). Events unfolded such a way that I always had to explore new areas of knowledge, combining it with the current duties. I think this kind of experience helps to work with new for me biomedical data. Visualization, manipulation of data and statistical analyses in the Data Analyst Nanodegree program were performed by me only in the fields of economics (finding strong correlations between market indicators), social studies (detect fraud at the company), or geo-regional studies (purification and extraction of useful information from open data).\n",
    "\n",
    "I worked twice in the process of preparing analysis-ready data files from source data, both times it started from zero to the working database in the end. In the Center of Standardization, I created the database alone. In the TNK-BP, we had the great team in the Development department and created the project data together (each member worked on his own areas and one operator of the database coordinated our activity).\n",
    "\n",
    "Producing dynamic and static data visualizations to help deliver the results of quantitative analysis to customers in innovative ways seems for me one of the most important parts of the duties. And I am happy that I have some variants of the geostatistical project with creating interactive data visualizations using the high-level programming packages (D3, Highcharts, etc.)\n",
    "\n",
    "Good programming skills I can demonstrate in the three of the mentioned languages (R, Python, JavaScript). I have programming experience with HTML, Shiny, Markdown, LaTeX as well. \n",
    "\n",
    "From all the above descriptions it is very easy to get an idea about the future prospects.\n",
    "\n",
    "The next goal is to complete at least one of the Nanodegree programs else: Machine Learning Engineer, Predictive Analytics for Business, Artificial Intelligence Engineer or VR Developer. I think to start with the course Machine Learning Engineer because it is the closest to the already finished course, but it has the next level of complexity. That is really great for the personal development.\n",
    "\n",
    "Ideally, a good specialist in the modern data processing should complete all 5 Nanodegree courses. There are several important reasons for that.\n",
    "- This is one of the fastest growing areas in the labor market. Without constant work on the skills, the candidate can not hope to get a good position.\n",
    "- A higher level of skills involves participation in the solution of the most interesting problems in the field of modern challenges (the creation of high technology for the processing of large data, the construction of virtual spaces for different purposes, the creation of a qualitatively new, preventive of the diseases, medicine, revolutionary changes in the natural sciences, the preservation of human values when creating different kind of artificial intelligence, etc.)\n",
    "- Excellent knowledge allows the candidate to show more creativity and flexibility in the process of working.\n",
    "\n",
    "As for me personally, I would like to qualitatively improve their skills in working with super large files, in the application of technologies JavaScript and HTML, in the construction of complex and interactive visualization, and the machine learning and then a year later  - in the building models of probability, natural language processing, computer vision. Also, I would like to expand the programming knowledge to the widest possible range and to use at least 5-7 languages in the nearest perspective. I hope I will have acquired the skills in the development of software packages for the implementation of specific programs with further work projects or Udacity training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "gist_id": "c302dd3875a0e05c4c4d39ce3796cb78",
  "hide_input": false,
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "latex_envs": {
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
